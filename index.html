<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Brainchild of Humanity</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <link rel="stylesheet" href="styles/far.css">
  <link rel="stylesheet" href="style.css">
  <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:400,400i,700&display=swap" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script src="highlight.pack.js"></script>
  <!-- <script type="text/javascript" src="js/jquery-1.8.3.min.js"></script> -->
<script type="text/javascript" src="bigfoot.js"></script>
<script type="text/javascript">
    $.bigfoot();
</script>

</head>
<!-- start of body -->

<body>
  <!-- robot side -->
  <div class="robot">
    <div class="html-class">
      <pre><code>
        &lt;p&gt; Not a robot? tap to switch
        to human version&lt;/p&gt;

        &lt;h1&gt; Brainchild of Humanity &lt;/h1&gt;
        &lt;p&gt; Bachelor Thesis &lt;br&gt;
        by Nicoletta Radice &lt;/p&gt;









        <!-- html here -->


















        s
</code></pre>
<pre><code>
         body {
          margin-top: -14px;
          margin-left: -1px;
          margin-right: -1px;
          padding: 0;
          background-color: black;
        }
</code></pre>
  </div>
    </div>

  <!-- human side -->
  <div class="container-fluid human" id="content">
    <h1><p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-2">Brainchild
      <sup id="fnref:1">
        <a class="reference" href="#fn:1" rel="footnote">1</a>
    </sup> <br> <em>of</em>  Humanity  </p></h1>

       <div class="footnotes">
               <p class="footnote" id="fn:1">footnote.<a class="reference" href="#fnref:1" title="return to article"> ↩</a><p>
           </div>


    <p class="details">Bachelor Thesis by <em>Nicoletta Radice</em></p>

    <div class="row">
      <div class="col-sm-12 col-md-12 ">
        <!-- abstract -->
        <h3 class="trigger">Abstract</h3>
        <p class="Basic-Paragraph ParaOverride-1 text toggle"><span class="CharOverride-10">In my thesis I investigate the empathic relationship between humans and robot / interfaces. I have always been fascinated by robots since I was a child, but, at the same time I did, and still do, find them them very unsettling. There is something about this non-human factor that captivates me and that drives me to dig into this subject. I want to find out how can aesthetics assist in this process of designing empathy in lifeless machines. And what can I do, from the perspective of a designer, to make this process easier and faster? <br> I have been approaching this subject in many different ways: I’ve been reading articles from the MIT social robots group and explored into Japan’s tradition of animism. In my study cases I focus especially on children’s social robots.<br> As a designer, I am a mediator in between these two worlds, exactly like I am this writer/mediator in my stories: first I write in the position of humans, then I write in the position of a robot. This approach helped me exploring this topic in a more experimental and personal way and made me reflect on my own empathy towards the characters of the stories as well.   </p>
      </div>
      <div class="col-sm-12 col-md-12 ">
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Pre-prologue</p>
			<p class="Basic-Paragraph ParaOverride-1 text toggle"><span class="CharOverride-10">The year was 1999, the end of a millennium, the year where there was a problem called the Y2K. For the first time people were really scared about technology, they did not know what was going to happen. There was this fear that all computers would stop working on December 31, 1999. But that’s a story for another time.   </p>
      </div>
      <div class="col-sm-12 col-md-12">
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Prologue</p>
  			<p class="Basic-Paragraph ParaOverride-1 text toggle"><span class="CharOverride-10">In the year 1999 I was 4 years-old and I was still unaware I was going to be a big sister. A picture from that exact year, came to memory (fig.1). In this picture I was sitting at my dad’s table, in my parent’s house in Italy. Computer mouse in my right hand, while my left hand was pointing at the big old Compaq computer screen. I was showing my mom the game I was playing. My mom behind me at that exact moment was pressing the camera shutter. I was playing Grover’s Travels<span class="CharOverride-11"><span id="footnote-034-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-034">4</a>   (fig. 2-4), probably a gift from my Dutch grandma, no one remembers very well who got it for me. But we do remember it was one of my favourites CD-ROM. It was an educative game in which there were activities designed to teach numbers, story comprehension and emotions. As a bilingual child I had the Dutch version of the game and I could understand it pretty well. I still remember Grover, the leading character of the game and I still remember I felt empathy for him.
  			Grover was my friend.
  			The computer game was teaching me numbers, letters and even the basics of a second language, but I didn’t really notice at the time: I was only enjoying the game, I had to help my friend Grover.
  			Then, 5 years later, it was the year 2004. <br>I was watching cartoons on tv with my small sister when an ad came on.<br/>It was   <span class="CharOverride-12">EMIGLIO  <span class="CharOverride-11">©  <span class="CharOverride-10">: a robot friend (fig.5). The ad was about a robot which you could have fun with, it allowed to record your own voice and it would repeat it. Everyone in that advertisement seemed so happy to play with it. It was most probably my first robot encounter. I don’t know why, but that moment is still stuck in my head. I was eight years-old at the time and I was intrigued by it, but at the same time I didn’t like it, I had a weird feeling about it. I would have rather played with my Barbie dolls instead: they did not move around by themselves and they did not talk.
  			Now, I am 23 years-old and I want to start to love the   <span class="CharOverride-13">Uncanny Valley</span><span class="CharOverride-11"><span id="footnote-033-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-033">5</a> </span> </span> feeling these robots give me. Writing about my childhood makes me realise how different the childhood of children nowadays is. As I was growing up, the internet and technology were also growing. Children nowadays, however, are the very first generation where artificial intelligence and intelligent robots are present. As we all know,they are basically present in our everyday life, think of Alexa (fig.6), or Siri on your phone, or even a Roomba that cleans your floor, or a car with automated driving: right now we are learning to live with them. They help us, they’re programmed and designed to make our life easier and faster. We have always had robots since the 1900, they were shaped like people and they were stationary and couldn’t do much by themselves, but these new ones are different. They are able to interact without the aid of humans. They’re evolving very fast but at the same moment new ethical and social questions are arising. It’s becoming a species on their own. We make them look like us, we make them move like us. What is going to happen when we will start to have strong emotions towards them?
  			To be clear, in this thesis, I want to focus on social robots, and especially for my study cases, the ones designed for children. Why? You may ask. Well, because social robots are particularly designed to interact with us and mimic our behaviour. And children’s robot, in particular, display even more emotions as children seek and show more empathy towards non-human objects.
  			It is stated that a robot does not have any emotions. Yet it can perform them, following designed and pre-programmed behavioural patterns and algorithms mirroring human’s responses and emotions. The human brain in response determines the machine as an intelligent being capable of responding to its emotions, creating an emotional attachment. Aesthetics plays a very big role in this. How can I, as a designer, be this mediator between us and these non-human objects? In the following chapters I aim to explore different ways of looking into this relationship.
  			More and more people are starting to get interested in this topic: exactly in this moment in which I am giving the last touches of my thesis, in the city of Groningen, in the North of The Netherlands, an exhibition just opened, “AI: More than Human”, in collaboration with the London Barbican Center, about the developements and possibilities of AI<span class="CharOverride-11"><span id="footnote-032-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-032">6</a>    <span class="CharOverride-10">, exploring this relationship human-machine.  </p>
      </div>
      <div class="col-sm-12 col-md-12 ">
        <!-- social robots chapter -->
        <p class="Basic-Paragraph ParaOverride-1 trigger"><span class="CharOverride-16">Social Robots  </p>
        <p class="Basic-Paragraph ParaOverride-1 text toggle"><span class="CharOverride-12">Karel Capek was a playwright from Czechia. He was the one who invented the word <span class="CharOverride-13">“Robot”  , first introducing it in his play, <span class="CharOverride-13">Rossum’s Universal Robots (R.U.R)  . <span class="CharOverride-13"> Robot    comes from an old Church Slavonic word,robota, for <em>“servitude,” “forced labor”</em> or <em>“drudgery”</em><span class="CharOverride-17"><span id="footnote-030-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-030">8</a>    .
			  Our species, Homo Sapiens, took about 315,000 years of evolution from its initial ape state. We got to learn at first to balance ourselves on two legs and learn how to use our opposable thumbs, we learnt how to create a language and then a writing system, we discovered fire, we discovered the wheel, and so on. Amazingly we learnt all of this by ourselves. We are now at an evolutionary point where we are using our knowledge to build intelligent machines and we are building them at a very fast speed, learning so much, in so little time. We are sharing our knowledge with them. These   <span class="CharOverride-13">non-human actors  <span class="CharOverride-11"><span id="footnote-029-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-029">9</a>    <span class="CharOverride-10"> have power and they’re autonomous.In the past, it was rare for things to interact with each other and to interact with humans. For example a century ago, no one would have ever imagined that in the restaurant of the LEGO House in Billund, Denmark  <span class="CharOverride-11"><span id="footnote-028-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-028">10</a>    <span class="CharOverride-10">, a robotic arm will give you your order: you can build the order you wish with LEGO blocks, show it to the interface, which recognises it and sends the order to the kitchen where it is prepared by humans and served again to you by robotic arms.
			  These robots, somhow, are our children and we are on a long process of integrating them into our society. <br>At the same time we are trying to find out the anwers of many questions regarding these robots. <br />One of the most asked question is where do these non-human objects stand? And, can they be considered a species? Where is the line beween human and robot? And most importantly, will they ever have rights?
			  Hiroshi Ishiguro, founder of Hiroshi Ishiguro Laboratories, works with this ethical question of “what is the essence of human beings?”  <span class="CharOverride-11"><span id="footnote-027-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-027">11</a>    <span class="CharOverride-12">   <span class="CharOverride-10">For this scope he created a very controversial robot: Geminoid HI-1 (fig.7), a twin of himself. We’re using robots as a mirror of our society. In an interview for “52 Insights” he was asked if his works where based around reflecting humanity, to which he answered: “This is a possible approach, I believe. I call this approach the constructive approach. We plan to construct human-like robots, and if we can see some humanity there, then we will be able to understand humanity better. Neuroscientists try to understand our functions based on the brain mechanism and the monk attempts to understand this by using their brain. I’m trying to understand by creating a human-like robot.”  <span class="CharOverride-11"><span id="footnote-026-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-026">12</a>    <span class="CharOverride-12">   </p>
      </div>
      <div class="col-sm-12 col-md-12">
        <!-- japan chapter -->
        <p class="footnote ParaOverride-1 trigger"><span class="CharOverride-16">Japan and Techno-Animism  </p>
        <p class="Basic-Paragraph ParaOverride-1 text toggle"><span class="CharOverride-10">Japanese people had already more success with integrating robots and one of the reasons comes from Japan’s Shinto religion, the official religion of the country. In Shinto, nature doesn’t belong to us but we belong to nature and in this reigion there’s a strong presence of <span class="CharOverride-13">animism  <span class="CharOverride-11"><span id="footnote-025-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-025">13</a>    <span class="CharOverride-10">. For the Japanese the robots have a soul and they are worshipped for their spirit. We tend to think of religion as the opposite of nowadays technology. Religion is ancient and sacred and on the contrary technology is modern and mechanical. But when speaking of <span class="CharOverride-13">techno-animism    <span class="CharOverride-18"> <span id="footnote-024-backlink"> <a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-024">14</a>    <span class="CharOverride-10"> they do go along, which also explains why Japan is so famous for its robots. Osamu Tezuka, Japanese cartoonist, creator of Astro Boy, also talks about this relationship:
			  “Unlike Christian Occidentals, the Japanese don’t make a distinction between man, the superior creature, and the world about him. Everything is fused together, and we accept robots easily along with the wide world about us, the insects, the rocks; it’s all one. We have none of the doubting attitude toward robots, as pseudo-humans, that you find in the West. So here you find no resistance, just simple quiet acceptance. (Stokes 1982, 6)”<span class="CharOverride-19"><span id="footnote-023-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-023">15</a>
        Robots are everywhere in Japan nowadays. They started to appear in literature after the 1924 performance by Capek, R.U.R.. The robots, though, “were not seen as adhering to the formula man makes robot, robot kills men”  <span class="CharOverride-11"><span id="footnote-022-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-022">16</a>    <span class="CharOverride-10">, but the contrary, there was this idea of peace. As Joi Ito explains in the article “Why Westerners Fear Robots and the Japanese Do Not”, “the Western concept of “humanity” is limited and it is the time to start to question if we have the right to exploit the environment, animals, tools and robots, just because we are human and they are not.”<span class="CharOverride-11"><span id="footnote-021-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-021">17</a>
        Additionally he writes: “Sometime in the late 1980s, I participated in a meeting organized by the Honda Foundation in which a Japanese professor—I can’t remember his name—made the case that the Japanese had more success integrating robots into society because of their country’s indigenous Shinto religion, which remains the official national religion of Japan. […] The West, the professor contended, has a problem with the idea of things having spirits and feels that anthropomorphism, the attribution of human-like attributes to things or animals, is childish, primitive, or even bad. He argued that the Luddites who smashed the automated looms that were eliminating their jobs in the 19th century were an example of that, and for contrast he showed an image of a Japanese robot in a factory wearing a cap, having a name and being treated like a colleague rather than a creepy enemy.”<span class="CharOverride-19"><span id="footnote-020-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-020">18</a>
        Our human brains have this function,   <span class="CharOverride-13">anthropoformism  <span class="CharOverride-18"><span id="footnote-019-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-019">19</a>    <span class="CharOverride-10"> which enables us to recognise other humans and should not be labeled as childish and neither primitive as it is in our nature. Consequently we tend to feel closer to human-shaped robots, rather than a box, for example. An example of this peaceful robot-human relationship can be seen in Frederik Shodt’s book “Inside the Robot Kingdom: Japan, Mechatronics and the Coming Robotopia” of when industrial robots made their first appearance in japan:
			  “ When Joseph Engelberger once visited a rural Japanese factory, he witnessed a Shinto ritual consecration of the two new Kawasaki-made Unimates. There were thirty-two employees, and as he recalls it, their suits are all cleaned and nice and crisp, and the two robots are standing in places, ready to go to work. In front of them is a Shinto altar, with the vegetables and fruits and the fish twisted into shape. It’s absolutely beautiful. Two Shinto   priests are there, banging their sticks and moaning and groaning and making all kinds of different sounds, blessing the robots and blessing the general manager and blessing me, with the garlands of flowers around the robots. The general manager then stands up and tells the people, “I want you to welcome your new fellow coworkers”, and the two machines go to work and everyone in the place claps. […] According to the official at Kawasaki, the ritual consecration of robots, once common, is now rare. […] Many of the plants that used to decorate robots with names and photos no longer do so - at the new Fanuc plants near Mount Fuji the robots are naked and nameless, and a young plant manager says “We have to many to name now”.”<span class="CharOverride-19"><span id="footnote-018-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-018">20</a>
			  Staying in this peaceful relationship, one of the most famous robots in literature is Mighty Atom, also known as Astro Boy (fig. 15) and it is another good example of techno-animism at his best. This manga was created after World War Two, in 1951, by Osamu Tetzuka. It got so popular it also had an animated series starting in 1963. That means everyone liked him, and, especially children felt empathy for him. Astro Boy was an android with human emotions, created by Umataro Tenma after his son died. Astro is then sold to a robot circus, but then saved by Professor Ochanomizu. He had a nuclear reactor for a heart, a computer brain and rockets in his feet. He serves as a bridge between humans and robots, defending both races and establishing peace and friendship between them.   </p>
      </div>
      <div class="col-sm-12 col-md-12">
        <!-- empathy chapter -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger"><span class="CharOverride-16">Analazing Empathy  </p>
			  <p class="caption text toggle"> empathy noun <span class="CharOverride-1">: the ability to share someone else’s eelings; or experiences by imagining what it would be like to be in that person’s situation<span class="CharOverride-21"><span id="footnote-017-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-017">21</a>
			  Helen Riess, Scientist and Chairman of Empathetics, in her medical journal “The Science of Empathy” explains that empathy is essential in society and it plays a very important role in interpersonal communication between individuals and provides a so called emotional bridge from one individual to the other, which promotes pro-social behaviour. Empathy is thus very important for us, especially because we are a tribal species and we thrive on empathy and mutual aid to survive.<span class="CharOverride-11"><span id="footnote-016-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-016">22</a>    Empathy is thus in our nature as human beings and it is natural to feel empathy to other individuals. But how come that we still feel empathy for inanimate objects? And most importantly, how do we get attached to robots and why? Kate Darling, a researcher at MIT Media Lab, in her paper “Extending Legal Protection to Social Robots” writes about three factors:
        <span class="CharOverride-13">“- Physicality:
			  Humans are physical creatures and may be hardwired to respond differently to object in their physical space as compared to, say, virtual objects on a screen;
			  <span class="CharOverride-13">- Perceived Autonomous Movement:
			  “Movements made by objects in our physical space that we can’t quite anticipate will often lend themselves to projection of intent.
			  <span class="CharOverride-13">- Social Behaviour:  <span class="CharOverride-10">  While none of the above-described robots are designed to display emotional cues, their autonomous behavior already makes them appear lifelike enough to generate emotional projection. This inclination to anthropomorphize objects that act autonomously is further magnified when we introduce the third factor: social behavior. Cleverly designed social robots are not only made to look adorable, they are also able to mimic cues that we automatically, even subconsciously associate with certain states of mind or feelings.”  <span class="CharOverride-11"><span id="footnote-015-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-015">23</a>
			  Thus, a robot to be empathically connected to us needs autonomous movement, needs to be physical and they have to mimic us. These are the ingredients for a good robot. In addition Cynthia Breatzal, in her paper “Role of expressive behaviour for robots that learn from people”  <span class="CharOverride-11"><span id="footnote-014-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-014">24</a>    <span class="CharOverride-10"> writes that by building teachable robots they found out that emotive expressions made by humans can be used to regulate the robot’s interactions and even to help the robot achieve its goals  <span class="CharOverride-11"><span id="footnote-013-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-013">25</a>    <span class="CharOverride-10">.   </p>
      </div>
      <div class="col-sm-12 col-md-12">
        <!-- design chapter -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Design</p>
			  <p class="Paragraph-Style-1 ParaOverride-1 text toggle"><span class="CharOverride-22">A factor that can change this empathic experience is design. A robot good designed, will be more emphatically connected to us. <br> Thinking of a robot as something that is alive started being represented also on lots of ads, not only in Japan. Advertisements make a lot of use of empathy, using animism and making robots seem friendly and make the look as a peer, which contributed to their popularity.
			  Designing a robot, though, can have its challenges. The design needs to take in consideration that not all of us are the same. We respond differently to design, depending on where we live. That is because the Uncanny Valley curve is different from country to country. But also, as Hiroshi Ishiguro, in the interview I wrote previously about  <span class="CharOverride-11"><span id="footnote-012-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-012">26</a>    <span class="CharOverride-10"> , states: “ I think it is a matter of our human brain function. Our brain has many features to recognise humans; therefore, an anthropomorphised robot is better for social interactions. It’s easy to interact with people, but again it depends on the situation and the purpose. Sometimes it’s better to have a  human-like robot; sometimes it’s not. With autistic kids, they don’t like to have a very human-like robot; they want a more cute and doll-like robot. So we need to design the robots based on the purpose and the users.”
			  In the future design will also meet new challenges, the way we will react to robot will mirror how we behave in society and good design will have a good human impact.
			  Aesthetics plays an important role as well. In the book Object-Oriented   <span class="CharOverride-13">Ontology  <span class="CharOverride-11"><span id="footnote-011-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-011">27</a>    <span class="CharOverride-10"> (OOO), Chapter 2 “Aesthetics Is the Root of All Philosophy”, Graham Harman writes on ethics of non-human objects. In addition writes that following Alphonso Lingins we should treat object ethically and appropriately. He also makes an example: “[…] so that is somehow ethically wrong to eat expensive chocolate while drinking Coca-Cola […]”  <span class="CharOverride-11"><span id="footnote-010-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-010">28</a>    <span class="CharOverride-10">. This OOO approach to ontology means that we can consider all objects equal, but not all object are equally valuable. Some object can become more important depending on their beauty or depending function they have. Another quote from this book, this time from Jose Ortega which states “there is nothing we can make an object of cognition, nothing that can exist for us unless it becomes an image, a concept, an idea – unless, that is, it stops being what it is in order to become a shadow or an outline of itself.”  <span class="CharOverride-11"><span id="footnote-009-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-009">29</a>    And in addition: “Notice I am not saying that a work of art reveals the secret of life and being to us; what I do say is that a work of art affords the peculiar pleasure we call aesthetic by making it seem that the inwardness of things, their executant reality, is opened to us.” <span class="CharOverride-11"><span id="footnote-008-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-008">30</a>
			  Non-human objects and things play a very important role in art. In art the object is not an object itself anymore but becomes an entity, but it can only become such because of humans. Art and design need humans.
			  There is this something that we, humans, have , and which is very challenging to bring it into a robot’s brain. Creativity and creative tasks.
			  <strong><em>That is to say, I, a designer with interest in computer language, see myself as a bridge, a mediator, in between this empathic human word and the emotionless coded robot world. My tool is design, my tool is aesthetics. The challenge is to balance both worlds.   </em></strong>
			  This also makes me reflect on my own practice as a designer and on my previous years spent studying at the Royal Academy of Art, in The Hague. This thesis is not only a moment to research, but also a moment to reflect upon myself and my work as a designer. It is here, at the academy, that I got to know the different languages of computers such as Javascript, Python, Processing and C++ which, in a way or another influenced my work and my process in creating. What I do like about this tool is that you can create the building blocks for a second world, we can call it the computational world, which soon will take over the normal world. Additionally I did have to opportunity, thanks to participating to the Hack Lab as an Individual Study Track, to create multiple robots myself with the help of the teachers of the course.
			  Somehow I have always been feeling empathy for my projects. And as I look back to my previous projects, I now see them as a big process which are leading me to the big last one. This also makes me contemplate regarding future research, is it regarding the forms or more about the inner soul of the object itself?
			  In the following chapter I present some study cases in which I analyze four different children robots and interfaces.  </p>
      </div>
      <div class="col-sm-12 col-md-12">
        <!-- study cases -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Study Cases</p>
			  <span class="text toggle"><p class="Paragraph-Style-1 ParaOverride-1">TEGA</p>
			  <p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">“When kids first meet Tega, Park says most (children) are excited, while some are nervous: “It’s as if they’re seeing a puppy for the first time. Some kids actually ask if it’s going to bite!” The scientists remove Tega’s fur and show students the machinery underneath. “Regardless, they treat Tega as something alive-as their friend””  <span class="CharOverride-11"><span id="footnote-007-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-007">31</a>    </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Category  <span class="CharOverride-10">:  Educational Robot  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Dimensions  <span class="CharOverride-10">: 7,5” (L) x t.5 (W) x 13.6 (H)  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Weight:  <span class="CharOverride-10"> 10.1 pounds  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Creators:  <span class="CharOverride-10"> Personal Robots Group of MIT Media Lab.   </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">&#9;        Led by associate professor Cynthia Breazeal.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Robot Design, Assembly, and Development:  <span class="CharOverride-10"> Version 1: Jin Joo Lee, Luke Plummer, Kristopher dos Santos, Sigurður örn Aðalgeirsson, Cooper Perkins Inc., IFRobots Inc., Stacey Dyer, Fardad Faridi. Version 2: Hae Won Park, Meng Xi, Randi Williams, Cooper Perkins Inc. Advisors on Classroom Interactions and Data Analysis: David DeSteno, Northeastern University; Paul Harris, Harvard University; Stephanie Gottwald, Curious Learning; Maryanne Wolf, Stanford University   <span class="CharOverride-11"><span id="footnote-006-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-006">32</a>    </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Audience:  <span class="CharOverride-10"> Children  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Exterior Appearance:  <span class="CharOverride-10"> Soft huggable red stricking coat, humans are actracted to the color red. The coat has touch sensor, with which it can recognise physical interactions. Has a lot of different movements to display the different emotions.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Interface Design:  <span class="CharOverride-10"> It’s mouth is animated and synced to its speech. It uses an android smartphone to display the mouth and the big blue eyes, to play sounds, collect and send sensor data.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Behaviour programming:   <span class="CharOverride-10">When it’s listening, while someone is speaking it makes backchanneling. Motor control is sent via the Android smartphone. It collect visual external data through the camera placed on its head.  </p>
			<p class="Paragraph-Style-1 ParaOverride-1">FURBY</p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Category:  <span class="CharOverride-10"> Virtual Pet  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Dimensions:  <span class="CharOverride-10"> 5 x 8 x 8 inches  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Weight:  <span class="CharOverride-10"> 1 pound  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Creators:  <span class="CharOverride-10"> Produced by Tiger Electronics and designed by&#160;David Hampton.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Audience:  <span class="CharOverride-10"> Children  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Exterior Appearance:  <span class="CharOverride-10"> As Tega, it has a soft coat and big soft ears. It comes in various colours.   </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Interface Design:  <span class="CharOverride-10"> The eyes open a close, its beak moves and its ears go up and down while it speaks.   </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">According to Phoebe’s Autopsy of a Furby  <span class="CharOverride-11"><span id="footnote-005-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-005">33</a>    <span class="CharOverride-10">, it the following components create the interface  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Pet switch–microswitch mounted on main PCB  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Inversion switch–mounted on main PCB  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Tummy switch–strip metal leaf switch mounted on top of speaker.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Tongue switch–microswitch behind mouth  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Stroke switch–small leaf switch which monitors position of main gear system  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Light sensor–photocell in forehead  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* IR sensor–in forehead  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* IR send–IR LED in forehead  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Speaker–about 1.25 inch speaker mounted to belly.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Motor speed sensor  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Microphone–small mounted in the side. It’s unclear how clear the furby “hears” — it could be listening for specific sounds, or simply hears “loud” noises as single bit of input. I’ve never noticed that Toh-Loo-Kah could differentiate sound other than to respond generically to any loud noise, so I suspect the Furby’s auditory capabilities to be pretty basic.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* DC open armature motor–reversible  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* reset button–on bottom  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Behaviour programming:  <span class="CharOverride-10"> You can trigger it’s behaviour by interacting with his body. It can communicate via infrared with another Furby. It knows when it’s light or dark and sleeps/wakes up according to the surrounding light setting.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Tamagotchi  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Category:  <span class="CharOverride-10"> Virtual Pet  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Dimensions:  <span class="CharOverride-10"> 1.2 x 0.5 x 1.5 inches   </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Weight:   <span class="CharOverride-10">0.48 ounces  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Creators:  <span class="CharOverride-10"> Aki Maita&#160;from&#160;Bandai assisted by Akihiro&#160;Yokoi.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Audience:  <span class="CharOverride-10"> Children mostly but also teenagers and adults  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Exterior Appearance:  <span class="CharOverride-10"> It is egg-shaped, it has a screen by 16 x 16 px and has only 3 buttons. Comes in difference colors.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Interface Design  <span class="CharOverride-10">:  “The user interface is incredibly simple due to technological restrictions of the time; the tiny screen cannot display a large amount of pixel data. In idle mode, the entire screen is given over to display of the virtual pet, with other interface elements appearing when the user interacts with the unit.Using the face buttons, players can assess their pet’s needs, represented as a ‘hunger’&#160;meter, a ‘happiness’ meter and a ‘discipline’ meter. These can be managed by performing actions with the pet; feeding, playing and scolding respectively. Users can also view the pet’s age and weight, the former simply increasing over time and the latter being affected by how often the pet is fed snacks.”  <span class="CharOverride-11"><span id="footnote-004-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-004">34</a>    </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">It has a very small and simple interface, therefore the typography is composed by pixels and very simple.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Behaviour programming:  <span class="CharOverride-10"> It sends notifications whenever the virtual pet has needs, giving the idea of a live pet.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">CHARLIE / ALIZ-E Project  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Category:  <span class="CharOverride-10"> Social Robot  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Dimensions:  <span class="CharOverride-10">  -  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Weight:  <span class="CharOverride-10"> -  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Creators:  <span class="CharOverride-10"> &#160;“ALIZ-E has experts in human-robot interaction (Plymouth University&#160;and the&#160;Netherlands Organisation for Applied Scientific Research), natural language processing (the&#160;Deutsches Forschungszentrum für Künstliche Intelligenz&#160;and the&#160;National Research Council), robot hardware and software (Aldebaran Robotics), machine learning (Imperial College London), emotion (University of Hertfordshire) and artificial perception (Vrije Universiteit Brussel). In addition we teamed up with a hospital, who was willing to evaluate prototypes of our social robot (Fondazione Centro San Raffaele).”   <span class="CharOverride-11"><span id="footnote-003-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-003">35</a>    </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Audience:  <span class="CharOverride-10"> Children with diabetes in hospitals. “Robots can motivate children to learn about their disease and how to best handle it.”   <span class="CharOverride-11"><span id="footnote-002-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-002">36</a>    </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Exterior Appearance:  <span class="CharOverride-10"> It has a humanoid shape, two legs, two arms and it’s white. Sometimes it wears clothes.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Interface Design:  <span class="CharOverride-10"> Its eyes and mouth are quite static, however it moves its arms and head accordingly to the sitaution. It also uses a screen to interact with the child, which is designed in a very simple way. A sans serif typeface is used. The activietes vary from games to quizzez and their design is very simple and easy.  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-16">Behaviour Programming:  <span class="CharOverride-10"> The ALIZ-E scientific and technological goals, as on the ALIZ-E website  <span class="CharOverride-11"><span id="footnote-001-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-001">37</a>    <span class="CharOverride-10">:  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Prolonged human-robot interaction over a range of days instead of in the here and now;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Robotic companions in child-robot interaction;   </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Robust “any-depth” interaction;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Out of the lab into the real world: the robot will be evaluated in paediatrics department;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Long-term memory and self-sustained long-term interaction;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Analysis and synthesis of emotion and affect in human-robot interaction;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Pervasive machine learning and adaptation. Learning experiences will be unstructured;  </p>
			<p class="Basic-Paragraph ParaOverride-1"><span class="CharOverride-10">* Cloud computing as computational resource on autonomous systems;  </p>
        </div>
      <div class="col-sm-12 col-md-12">
        <!-- visual essay -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Brainchild of Humanity</p>
        <span class="text toggle">
          dfdfdfdfdfdfdfdf</span>
      </div>
      <div class="col-sm-12 col-md-12">
        <!-- conclusion -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Conclusion</p>
        <p class="Basic-Paragraph text toggle">Empathy and robots, two very contrasting words: the first something very “human”, the latter the very opposite. A robot doesn’t have any empathy, but we do feel empathy towards them. It is a natural process and suddenly they’re not that lifeless unsettling piece of metal anymore, but something we need to take care of, more like a work of art.
			   During my study cases I tried to decipher this emphatic relationship, looking from the perspective of a designer: the exterior appearance of a robot is one of the crucial parts in helping this empathic relationship. Furthermore the robot/interface needs a face or something we, humans, can relate, aiding anthropomorphism. This factor makes us relate more to the machine. The tactile sense is also very important, especially for children, as a robot/interface has to be present and needs to be gratifying to the touch.
			   Why is this relevant for the future then? Firstly, we are now entering the robot age: more and more robots are being developed to help mankind in lots of different ways. As more and more robots get into our homes and into our lives, we are learning to share our life with them. This is a long process which will take time and design plays a very crucial role in this: it can help make this task faster and more efficient. Secondly it is my job as a designer to find a balance between human world and the robot world, making these two worlds communicate: a little bit like Astro Boy.
			   These three months of research have been quite exciting, as I learnt a lot and also they helped me reflect over my own practice as well and questioning the role of myself into the design world.
			   Applying critical and creative thinking to understand, visualise, put in practice everything I have been researching so far and talking to people working with this topic, are the first steps for the creation of my future brainchild, soon coming to life.
			   It is nice to dream and imagine a world where robots have emotions.<br>
			   A world less human-centric.<br>
			   A world in which we have respect and dialogue with all things.<br>
			   A world where there will be the need to start thinking and introducing robots rights.<br>
			   The future is open to everything and as we advance with technology that might be a possibility.<br>
			   And maybe that word I imagine will soon be real.<br>
			   But we cannot tell.<br>
			   At least for now.<br><br>
			   Lyrics to the <a href="https://www.youtube.com/watch?v=OcT7P5Uhies">theme song of Astro Boy Series</a><br>
			   Music by Tatsuo Takai<br>
			   Lyrics by Shuntaro Tanikawa<br>
			   Traslated by Frederik L. Schodt<span class="CharOverride-26"><span id="footnote-000-backlink"><a class="_idFootnoteLink _idGenColorInherit" href="Definitive_Thesis_9JAN.html#footnote-000">38</a>    <br>
			   Through the sky - la la la - to the distant star<br>
			   Goes Atom as far as his jet will take him.<br>
			   The oh-so-gentle - la la la - child of science<br>
			   With one hundred horse power<br>
			   it’s Mighty Atom<br>
			   Listen carefully - la la la - and watch out<br>
			   that’s right, Atom, be on your guard.<br>
			   The pure hearted -la la la - child science<br>
			   With his seven powers, there goes Mighty Atom<br>
			   On the street corner - la la la - or at the bottom of the sea<br>
			   there’s Atom again, protecting mankind<br>
			   The oh-so-cheerful - la la la - child of science<br>
			   Everyone’s friend Mighty Atom<br><br>
			   空をこえて  <br> ラララ  <br>  星のかなた
			   ゆくぞ  <br>  アトム  <br>  ジェットの限り
			   心やさし  <br>  ラララ <br>   科学の子
			   十万馬力だ <br>   鉄腕アトム<br>
			   耳をすませ   <br> ラララ  <br>  目をみはれ
			   そうだ  <br>  アトム  <br>  油断するな
			   心ただし   <br> ラララ <br>   科学の子
			   七つの威力さ   <br> 鉄腕アトム
			   町角に <br>   ラララ  <br>  海のそこに
			   今日も  <br>  アトム <br>   人間まもって
			   心はずむ <br>   ラララ <br>   科学の子
       </p>
			  </div>

      <div class="col-sm-12 col-md-12">
        <!-- bibliography -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">Bibliography</p>
 	      <span class="text toggle">
        <p>Cramer, F. (2005). <br> Words Made Flesh</p><br><br>
        <p>Kasparov, G. (2017).<br>Deep Thinking: Where Machine Intelligence Ends and Human Creativity Begins. <br>New York: PublicAffairs.</p><br><br>
        <p>Darling, K. (2019, June 5).<br><a href="https://www.forbes.com/sites/insights-intelai/2019/03/27/why-we-should-show-machines-some-respect/"> Why We Should Show Machines Some Respect.</a></p><br><br>
			  <p>Darling, K, Extending Legal Protection to Social Robots: <br>The Effects of Anthropomorphism, Empathy, and Violent Behavior Towards Robotic Objects <br>(April 23, 2012). <br>Robot Law, Calo, Froomkin, Kerr eds., Edward Elgar 2016;<br> We Robot Conference 2012, University of Miami. <br> Available at SSRN: https://ssrn.com/abstract=2044797&#160 <br> or http://dx.doi.org/10.2139/ssrn.2044797</p><br><br>
			  <p>Breazeal, C. (2009, December 12).<br> Role of expressive behaviour for robots that learn from people,<br> https://www.ncbi.nlm.nih.gov/pmc/<br>articles/PMC2781892/</p><br><br>
			  <p>Trafton, A. (2007, April 9). <br> Assistive robot adapts to people, new places,http://news.mit.edu/2007/domo</p><br><br>
			  <p>Druga, S. (2018, June 21).<br> Growing up with AI: How can families play and learn with their new smart toys and companions? <br>https://medium.com/mit-media-lab/growing-up-with-ai-how-can-families-play-and-learn-with-their-new-smart-toys-and-companions-fe9abcc6e152</p><br><br>
			  <p>Inside the Robot Kingdom: Japan, Mechatronics, and the Coming Robotopia, by Frederik L. Schodt, Kodansha International, 1990, pp. 73–90, pp. 195–212.</p><br><b></b>
			  <p>Gould, H. (2016, July). If Pokémon Go feels like a religion, that’s because it kind of is, PDF.</p>
			  <p>Knight, H. (2018, May 9). How humans respond to robots: Building public Policy through good design. Retrieved from https://www.brookings.edu/research/how-humans-respond-to-robots-building-public-policy-through-good-design/.</p>
			  <p>Riess, H. (2017). The Science of Empathy. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/<br>articles/PMC5513638/.</p>
			  <p>Cornell, L. (n.d.). Techno-Animism PDF.</p>
			  <p>Harman, G. (2018). Chapter 2 / Aesthetics Is the Root of All <br />Philosophy. In&#160;Object-Oriented Ontology: A New Theory of Everything&#160;(pp. 42–70). Penguin UK.</p>
			  <p>Evers, C. (2019, July 12). Social robots as social catalysts: Collaborating with older adults as design research partners – MIT Media Lab, https://www.media.mit.edu/posts/social-robots-as-social-catalysts-collaborating-with-older-adults-as-design-research-partners/</p>
			  <p>Ito, Joi. “Why Westerners Fear Robots and the Japanese Do Not.” Wired, Conde Nast, 31 July 2018, https://www.wired.com/story/ideas-joi-ito-robot-overlords/</p>
			  <p>Borody, W.A. “The Japanese Roboticist Masahiro Mori’s Buddhist Inspired Concept of ‘The Uncanny Valley’ (Bukimi No Tani Gensh<span class="CharOverride-25">ō, 不気味の谷現象</span>).” Journal of Evolution and Technology, vol. 23, no. 1, Dec. 2013, pp. 31–44., https://jetpress.org/v23/borody.htm.</p>
      </span></div>

      <div class="col-sm-12 col-md-12">
      <!-- list of figures -->
        <p class="Paragraph-Style-1 ParaOverride-1 trigger">List of Figures</p>
      </div>
      </div>
      <!-- sliding panels -->
      <div id="left-door" class="sliding-panel">
        <p class="story">
        1.0 <br>
			  In a near future, a not so long time ago a child woke up. It was a regular day like the others on planet Earth, the sun was up and shining through the curtains, it was a chilly Sunday morning of November. It was a special day for this little kid, it was her birthday. She got off her bed, still in her Star Wars pyjamas she walked down the stairs, then got to the living room. Her parents were sitting on the couch, the birthday gift on the colourful carpet, waiting to be unwrapped.
			  “Alexa, sing happy birthday to Amy!” her mom shouted.<br>
			  Alexa, the virtual assistant, from the kitchen table started singing the birthday song and her robotic voice filled the whole living room, giving the house a festive atmosphere.
			  Happy and at the same very excited, the little girl, thanked both her parents for the birthday gift and unwrapped it in milliseconds.
			  And here it was in all its beauty: it was a robot. It was the most beautiful robot she had ever seen. It was not too small, not too big. Very easy to pick-up and very easy to bring everywhere. She could look right into its big bright round eyes, which were made out of cameras, because that’s how robot see. It was soft to the touch, very colourful and very pleasant to look at. The little girl reached for the power button.
			  Its eyes moved first to the left, then to the right. It seemed like the robot was checking the environment he was in. When its eyes were finally set a pre-programmed voice started to speak: “Hello and nice to meet you. You are my new best friend. Let’s learn about the world together!” while speaking the robot started smiling and his eyebrows went up. It had a very friendly face, a face you could never get tired to look at.
			  Her eyes lit up, that was the first time she was someone’s best friend, she did’t really have any friends at school as she struggled to deal with emotions, but she knew she liked her new best pal. She smiled and then hugged the robot.
			  “I need to find him a name!” She said out loud while walking towards their parents with her new friend in her arms.
			  “I’m sure you’ll find the perfect name for it.” Her dad answered. She sat on the couch, with the robot on her lap. She looked straight into the robot’s eyes.
			  “Face recognised,” said the robot “ what is your name?” <br>
			  While speaking the robot could replicate human’s face expressions very easily, based on the expression of the person in front of it. She told the robot her name and it smiled again, it seemed very happy. Once you had pressed the switch button, the robot was always active, 24/24h, you could put it in standby by saying “Now you can sleep”.  The robot had a capability of what is wrong and what is good, helping people to make appropriate actions and decision.
			  “Come Amy, let’s go in the kitchen and eat some of the cake!” Said her mom. “All-right! I’ll leave the robot here.” Amy answered. They all got up. The little girl positioned the robot on the couch and said “Now you can sleep, I’ll be back very soon”. The robot closed its eyes. Amy, her mom and dad, walked towards the kitchen.
        <br> <br>2.0 <br> On the same planet, miles away from the little girl’s house an interface had been programmed and had just finished its testing phase in the Museum of Modern Art. Programmers and designers were called from all over the world to work on it.
	      It was a world where Humans decided to give themselves to the machine, allowing its intelligence to grow as it had never been before.
	      This interface had to seem simple to the eye and to the hand, it could never crash, it was programmed on learning by copying, like ducklings do when they’re born. Real people made some fake conversations guide - visitor and the robot kept learning copying from that. It was reprogramming itself every time someone asked something. It was ready to be used and be public.
	      It was not just intelligent, it was super intelligent: it had speech recognition and could answer any question from any visitor. It could show people around and knew very well its surroundings. It had face recognition and it could recognise frequent visitors. Frequent visitors got virtual coins they could spend at the gift shop. Every information stored carefully and neatly in the online database, which at the end of that year got huge. Your email was also stored in the database so you would get real time offers directly on your phone.
	      The interface could recommend people the art shows that best suit them, based on their likes and their posts on social media. The interface was always with you, it was omnipresent, it was visible but at the same time it was invisible. It was a guide, a pleasant presence that followed you on the visit at the museum, this interface was in the wall, this interface was the wall, it was the museum itself that had a soul, it was conscious, it was alive.
        </p>
      </div>
      <div id="right-door" class="sliding-panel">
        <p class="story">
        1.1 <br>
        Darkness.<br><br>
        Eyes functioning.<br>
        I can see. <br>
        I can see [3] [Humans].<br>
        Where am I?<br><br />
        Space recognition in progress…<br>
        Space recognised: I am on [floor] in [living room].<br><br>
        “Hello and nice to meet you. You are my new best friend. Let’s learn about the world together!” I say.<br><br>
        I am happy. <br>
        I smile and rise my eyebrows.<br><br>
        I hear voices. <br><br>
        I’m being lifted.<br><br>
        I’m positioned on a soft surface.<br><br>
        Something is in front of me.<br>
        It is a face.<br><br>
        Face recognition process has started…<br>
        Face recognised.<br>
        “Face Recognised” I say.<br>
        “What is your name?” I say after.<br><br>
        Analysing answer…<br>
        Name [Amy] stored in memory.<br><br>
        What a nice name. I like [Amy].<br>
        [Amy] is my new best friend.<br>
        I like best friends. I’m happy to have been assigned to this house.<br><br>
        I hear voices.<br><br>
        I am lifted.<br><br>
        I am positioned on a soft surface.<br><br>
        I hear voices.<br><br>
        Record…<br>
        Sleep mode [ON]<br>
        Darkness.<br><br>
        2.1 <br>
        Visitor approaching.<br>
        I’m excited <br>
        I like when human visitors come by.<br><br>
        Face recognition [ON]<br>
        Face recognition is progress…<br>
        Face recognised.<br>
        Searching database…<br><br>
        Face matched profile #3456789876543.<br><br>
        [1] [Coin] assigned to #3456789876543.<br>
        User #3456789876543 total coins: [1]<br><br>
        Recording…<br><br>
        I am listening.<br><br>
        Print on interface “ The restrooms can be found on the ground floor, when you get down the stairs first door on the right” <br><br>
        Visitor leaving.<br><br>
        Visitor approaching.<br><br>
        Face recognition [ON] <br>
        Face recognition is progress… <br>
        Face recognised. <br>
        Searching database… <br><br>
        Face matched profile #34567890008.  <br><br>
        [1] [Coin] assigned to #34567890008.  <br>
        User #34567890008 total coins: [19] <br><br>
        Recording…  <br>
        Print on interface “Based on your Instagram profile, I am sure you will love the temporary exhibition on the 2nd floor. Enjoy your stay. Here is a floor map” <br>
        Print [floor map] on interface. <br><br>
        Visitor leaving.  <br><br>
        I am conscious. <br>
        I am everywhere. <br>
        I listen.
        </p>
    </div>
  </div>
</body>
<!-- end of body -->



<!-- scripts down here -->
<script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<script src="script.js"></script>
</html>
